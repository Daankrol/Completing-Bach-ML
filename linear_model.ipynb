{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_generator import WindowGenerator\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from read_data import *\n",
    "from process_data import *\n",
    "from methods import *\n",
    "MAX_EPOCHS = 20\n",
    "VOICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, patience=3, verbose=0):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=patience,\n",
    "                                                      mode='min',\n",
    "                                                     restore_best_weights=True)\n",
    "#  Early stopping when there is no improvement in loss. \n",
    "    model.compile(loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.CategoricalAccuracy(), tf.metrics.CategoricalCrossentropy(from_logits=True),tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# At the end of each epoch, the model will iterate over the validation dataset and compute the validation loss and validation metrics.\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/Daan/Documents/Study/MSc/Machine Learning/project/Completing-Bach-ML/window_generator.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.train_df.iloc[:,:5] = (self.train_df.iloc[:,:5] - self.mean_train) / self.std_train\n",
      "/Users/Daan/Documents/Study/MSc/Machine Learning/project/Completing-Bach-ML/window_generator.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.val_df.iloc[:,:5] = (self.val_df.iloc[:,:5] - self.mean_train) / self.std_train\n",
      "/Users/Daan/Documents/Study/MSc/Machine Learning/project/Completing-Bach-ML/window_generator.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.test_df.iloc[:,:5] = (self.test_df.iloc[:,:5] - self.mean_train) / self.std_train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (20, 54, 25)\n",
      "Labels shape (batch, time, features): (20, 20)\n",
      "<MapDataset shapes: ((None, 54, 25), (None, 20)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "multi_step_window = WindowGenerator(input_width=54, label_width=1, shift=1, batch_size=20, voice_number=VOICE)\n",
    "\n",
    "for example_inputs, example_labels in multi_step_window.train.take(1):\n",
    "    print(f\"Inputs shape (batch, time, features): {example_inputs.shape}\")\n",
    "    print(f\"Labels shape (batch, time, features): {example_labels.shape}\")\n",
    "\n",
    "print(multi_step_window.train)\n",
    "# print(len(multi_step_window.label_columns_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (20, 54, 25)\n",
      "Output shape: (20, 20)\n",
      "Epoch 1/20\n",
      "103/103 - 5s - loss: 1.2552 - categorical_accuracy: 0.7936 - categorical_crossentropy: 1.2552 - mean_absolute_error: 2.0875 - val_loss: 0.7769 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.7769 - val_mean_absolute_error: 2.8304\n",
      "Epoch 2/20\n",
      "103/103 - 2s - loss: 0.7156 - categorical_accuracy: 0.8034 - categorical_crossentropy: 0.7156 - mean_absolute_error: 3.3928 - val_loss: 0.7258 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.7258 - val_mean_absolute_error: 3.6926\n",
      "Epoch 3/20\n",
      "103/103 - 1s - loss: 0.6356 - categorical_accuracy: 0.8098 - categorical_crossentropy: 0.6356 - mean_absolute_error: 4.2039 - val_loss: 0.7249 - val_categorical_accuracy: 0.8223 - val_categorical_crossentropy: 0.7249 - val_mean_absolute_error: 4.0409\n",
      "Epoch 4/20\n",
      "103/103 - 1s - loss: 0.5776 - categorical_accuracy: 0.8186 - categorical_crossentropy: 0.5776 - mean_absolute_error: 4.6245 - val_loss: 0.7146 - val_categorical_accuracy: 0.8187 - val_categorical_crossentropy: 0.7146 - val_mean_absolute_error: 4.3228\n",
      "Epoch 5/20\n",
      "103/103 - 1s - loss: 0.5224 - categorical_accuracy: 0.8367 - categorical_crossentropy: 0.5224 - mean_absolute_error: 4.9933 - val_loss: 0.7246 - val_categorical_accuracy: 0.8132 - val_categorical_crossentropy: 0.7246 - val_mean_absolute_error: 4.7452\n",
      "Epoch 6/20\n",
      "103/103 - 1s - loss: 0.4589 - categorical_accuracy: 0.8533 - categorical_crossentropy: 0.4589 - mean_absolute_error: 5.4206 - val_loss: 0.7157 - val_categorical_accuracy: 0.8187 - val_categorical_crossentropy: 0.7157 - val_mean_absolute_error: 5.1479\n",
      "Epoch 7/20\n",
      "103/103 - 1s - loss: 0.4008 - categorical_accuracy: 0.8738 - categorical_crossentropy: 0.4008 - mean_absolute_error: 5.8269 - val_loss: 0.7397 - val_categorical_accuracy: 0.8168 - val_categorical_crossentropy: 0.7397 - val_mean_absolute_error: 5.5036\n",
      "Note that these are the LAST model metrics and not the BEST model metrics because of early stopping with restore_best_weights \n",
      "epochs: 7, loss: 0.4008, mean_absolute_error: 5.8269, val_loss: 0.7397, val_mean_absolute_error: 5.5036\n",
      "Evaluate\n",
      "13/13 [==============================] - 1s 12ms/step - loss: 1.2408 - categorical_accuracy: 0.6423 - categorical_crossentropy: 1.2408 - mean_absolute_error: 4.5596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.2408424615859985,\n",
       " 'categorical_accuracy': 0.642276406288147,\n",
       " 'categorical_crossentropy': 1.2408424615859985,\n",
       " 'mean_absolute_error': 4.559627056121826}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_output_nodes = len(multi_step_window.label_columns_indices)\n",
    "\n",
    "initializer = tf.keras.initializers.RandomUniform(minval=-0.01, maxval=0.01)\n",
    "linear_multi_step = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(units=64, kernel_initializer=initializer, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, kernel_initializer=initializer, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=n_output_nodes, kernel_initializer=initializer),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "#     tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "print('Input shape:', multi_step_window.example[0].shape)\n",
    "print('Output shape:', linear_multi_step(multi_step_window.example[0]).shape)\n",
    "\n",
    "# Train the model\n",
    "history = compile_and_fit(linear_multi_step, multi_step_window, verbose=2)\n",
    "\n",
    "print('Note that these are the LAST model metrics and not the BEST model metrics because of early stopping with restore_best_weights ')\n",
    "print(\n",
    "    \"epochs: {}, loss: {:0.4f}, mean_absolute_error: {:0.4f}, val_loss: {:0.4f}, val_mean_absolute_error: {:0.4f}\".format(\n",
    "        len(history.history['loss']),\n",
    "        history.history[\"loss\"][-1],\n",
    "        history.history[\"mean_absolute_error\"][-1],\n",
    "        history.history[\"val_loss\"][-1],\n",
    "        history.history[\"val_mean_absolute_error\"][-1],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluate\")\n",
    "result = linear_multi_step.evaluate(multi_step_window.test)\n",
    "dict(zip(linear_multi_step.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (20, 54, 25)\n",
      "Output shape: (20, 20)\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 7s 43ms/step - loss: 1.7743 - categorical_accuracy: 0.6446 - categorical_crossentropy: 1.7743 - mean_absolute_error: 0.6247 - val_loss: 0.8479 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.8479 - val_mean_absolute_error: 1.6101\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 3s 34ms/step - loss: 0.8613 - categorical_accuracy: 0.8102 - categorical_crossentropy: 0.8613 - mean_absolute_error: 1.6558 - val_loss: 0.8375 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.8375 - val_mean_absolute_error: 1.8479\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 3s 31ms/step - loss: 0.8538 - categorical_accuracy: 0.8032 - categorical_crossentropy: 0.8538 - mean_absolute_error: 1.8278 - val_loss: 0.8301 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.8301 - val_mean_absolute_error: 1.9236\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 4s 36ms/step - loss: 0.8090 - categorical_accuracy: 0.8113 - categorical_crossentropy: 0.8090 - mean_absolute_error: 1.9428 - val_loss: 0.8192 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.8192 - val_mean_absolute_error: 1.9693\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 4s 34ms/step - loss: 0.7922 - categorical_accuracy: 0.8141 - categorical_crossentropy: 0.7922 - mean_absolute_error: 1.9776 - val_loss: 0.8020 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.8020 - val_mean_absolute_error: 2.0537\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 4s 37ms/step - loss: 0.7710 - categorical_accuracy: 0.8142 - categorical_crossentropy: 0.7710 - mean_absolute_error: 2.0318 - val_loss: 0.7909 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.7909 - val_mean_absolute_error: 2.0737\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 3s 31ms/step - loss: 0.7355 - categorical_accuracy: 0.8132 - categorical_crossentropy: 0.7355 - mean_absolute_error: 2.0477 - val_loss: 0.7075 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.7075 - val_mean_absolute_error: 2.1829\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 3s 30ms/step - loss: 0.6648 - categorical_accuracy: 0.8095 - categorical_crossentropy: 0.6648 - mean_absolute_error: 2.1851 - val_loss: 0.6929 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.6929 - val_mean_absolute_error: 2.3006\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 5s 44ms/step - loss: 0.6484 - categorical_accuracy: 0.8192 - categorical_crossentropy: 0.6484 - mean_absolute_error: 2.2867 - val_loss: 0.6934 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.6934 - val_mean_absolute_error: 2.3602\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 5s 45ms/step - loss: 0.6525 - categorical_accuracy: 0.8187 - categorical_crossentropy: 0.6525 - mean_absolute_error: 2.2908 - val_loss: 0.6891 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.6891 - val_mean_absolute_error: 2.3956\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 4s 39ms/step - loss: 0.6138 - categorical_accuracy: 0.8245 - categorical_crossentropy: 0.6138 - mean_absolute_error: 2.3502 - val_loss: 0.7092 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.7092 - val_mean_absolute_error: 2.5490\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 4s 43ms/step - loss: 0.6315 - categorical_accuracy: 0.8204 - categorical_crossentropy: 0.6315 - mean_absolute_error: 2.4087 - val_loss: 0.6787 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.6787 - val_mean_absolute_error: 2.4635\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 3s 31ms/step - loss: 0.6092 - categorical_accuracy: 0.8193 - categorical_crossentropy: 0.6092 - mean_absolute_error: 2.4210 - val_loss: 0.6884 - val_categorical_accuracy: 0.8242 - val_categorical_crossentropy: 0.6884 - val_mean_absolute_error: 2.5287\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 4s 38ms/step - loss: 0.6044 - categorical_accuracy: 0.8253 - categorical_crossentropy: 0.6044 - mean_absolute_error: 2.4637 - val_loss: 0.6834 - val_categorical_accuracy: 0.8168 - val_categorical_crossentropy: 0.6834 - val_mean_absolute_error: 2.4920\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 4s 35ms/step - loss: 0.5976 - categorical_accuracy: 0.8233 - categorical_crossentropy: 0.5976 - mean_absolute_error: 2.5108 - val_loss: 0.6943 - val_categorical_accuracy: 0.8132 - val_categorical_crossentropy: 0.6943 - val_mean_absolute_error: 2.5574\n",
      "Note that these are the LAST model metrics and not the BEST model metrics because of early stopping with restore_best_weights \n",
      "epochs: 15, loss: 0.6165, mean_absolute_error: 2.4956, val_loss: 0.6943, val_mean_absolute_error: 2.5574\n",
      "Evaluate\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 1.1562 - categorical_accuracy: 0.6911 - categorical_crossentropy: 1.1562 - mean_absolute_error: 2.3962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.1562236547470093,\n",
       " 'categorical_accuracy': 0.6910569071769714,\n",
       " 'categorical_crossentropy': 1.1562236547470093,\n",
       " 'mean_absolute_error': 2.3961985111236572}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_output_nodes = len(multi_step_window.label_columns_indices)\n",
    "\n",
    "rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(units=n_output_nodes),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "#     tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "print('Input shape:', multi_step_window.example[0].shape)\n",
    "print('Output shape:', rnn(multi_step_window.example[0]).shape)\n",
    "\n",
    "# Train the model\n",
    "history = compile_and_fit(rnn, multi_step_window, verbose=1)\n",
    "\n",
    "print('Note that these are the LAST model metrics and not the BEST model metrics because of early stopping with restore_best_weights ')\n",
    "print(\n",
    "    \"epochs: {}, loss: {:0.4f}, mean_absolute_error: {:0.4f}, val_loss: {:0.4f}, val_mean_absolute_error: {:0.4f}\".format(\n",
    "        len(history.history['loss']),\n",
    "        history.history[\"loss\"][-1],\n",
    "        history.history[\"mean_absolute_error\"][-1],\n",
    "        history.history[\"val_loss\"][-1],\n",
    "        history.history[\"val_mean_absolute_error\"][-1],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluate\")\n",
    "result = rnn.evaluate(multi_step_window.test)\n",
    "dict(zip(rnn.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last window\n",
    "def predict_new_pitches(model, n=100):\n",
    "    last_window = None\n",
    "    last_pitch = None\n",
    "    all_predicted_pitches = []\n",
    "    all_probs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if last_window is None:\n",
    "            # Use an iterator to get the last window of the dataset. \n",
    "            for last_window, labels in multi_step_window.test_no_shuffle.as_numpy_iterator():\n",
    "                pass\n",
    "\n",
    "        # Add dimension to input\n",
    "        probabilities = model.predict(np.array([last_window]))[0]\n",
    "        all_probs.append(np.array(probabilities))\n",
    "        \n",
    "        predicted_shift = get_shift_from_probability(\n",
    "            probabilities,\n",
    "            key=multi_step_window.shift_conversion_key,\n",
    "            method=SelectionMethod.PROB, n=10,\n",
    "        )\n",
    "        \n",
    "        # Get latest pitch of the dataset\n",
    "        if last_pitch is None:\n",
    "            last_pitch = get_voice(VOICE)[-1]\n",
    "            \n",
    "        predicted_pitch = last_pitch + predicted_shift\n",
    "        all_predicted_pitches.append(predicted_pitch)\n",
    "        \n",
    "        pitch_features = get_pitch_features(predicted_pitch, VOICE)\n",
    "        shift_one_hot = [0] * len(multi_step_window.shift_conversion_key)\n",
    "        shift_one_hot[multi_step_window.shift_conversion_key.index(predicted_shift)] = 1\n",
    "\n",
    "        last_pitch = predicted_pitch\n",
    "\n",
    "        # create new dataframe\n",
    "        new_df = pd.DataFrame(dtype=float, index=[0])\n",
    "        (\n",
    "            new_df[\"log_pitch\"],\n",
    "            new_df[\"chroma_x\"],\n",
    "            new_df[\"chroma_y\"],\n",
    "            new_df[\"c5_x\"],\n",
    "            new_df[\"c5_y\"],\n",
    "        ) = pitch_features\n",
    "        \n",
    "        # - Normalise features\n",
    "        new_df = (new_df - multi_step_window.mean_train) / multi_step_window.std_train\n",
    "        features = new_df.values.tolist()[0] + shift_one_hot\n",
    "        # - Make new window based on previous timesteps with a slide of 1 and the normalised features\n",
    "        last_window = np.append(last_window[1:], [features], axis=0)\n",
    "    return all_predicted_pitches, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 67, 67, 65, 65, 66, 66, 65, 65, 70, 70, 71, 71, 71, 71, 73, 73, 71, 71, 70, 70, 68, 68, 66, 66, 66, 66, 64, 64, 63, 63, 62, 62, 60, 60, 59, 59, 61, 61, 60, 60, 58, 58, 60, 60, 60, 60, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 56, 56, 61, 61, 61, 61, 54, 54, 54, 54, 56, 56, 54, 54, 56, 56, 54, 54, 59, 59, 59, 59, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62]\n"
     ]
    }
   ],
   "source": [
    "# print(predict_new_pitches(linear_multi_step, n=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "probability_model = tf.keras.Sequential([linear_multi_step, tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions, probs = predict_new_pitches(probability_model, n=100)\n",
    "print(predictions)\n",
    "\n",
    "# # print(probs[-1])\n",
    "# # sum(probs[-1])\n",
    "# # # wg = WindowGenerator(input_width=6, label_width=1, shift=1, batch_size=1)\n",
    "# # print(f'train {sum(1 for _ in wg.train)} val {sum(1 for _ in wg.val)} test {sum(1 for _ in wg.test)} ')\n",
    "# # print(f'full dataset {sum(1 for _ in wg.full_dataset)}')\n",
    "# # print(f'data size {wg.df.shape[0]} | file voice size: {len(get_voice(0))}')\n",
    "\n",
    "# # max(get_voice(2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
